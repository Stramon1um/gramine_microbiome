{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff4e3dd-d99c-46a3-897b-e9874dc79ac0",
   "metadata": {},
   "source": [
    "# DADA2 ASV pipeline\n",
    "\n",
    "This notebook uses the [DADA2](https://benjjneb.github.io/dada2/index.html) pipeline to infer ASVs from 16S rhizosphere samples.\n",
    "\n",
    "## Setup\n",
    "\n",
    "The analysis was performed using dada2 1.10.0 with R 3.5.1. The computational environment used to carry out the analysis can be replicated on a Linux system using [conda](https://docs.conda.io/en/latest/miniconda.html). Download the miniconda installer [here](https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh) then follow the [installation instruction](https://docs.conda.io/en/latest/miniconda.html#installing).\n",
    "\n",
    "Once conda is available on your system, the dada2 environment can be created by running the following command from within the directory containing this notebook:\n",
    "\n",
    "```\n",
    "conda env create -f dada2_1.10.0.conda.yaml\n",
    "```\n",
    "\n",
    "The conda environment can then be activate using \n",
    "\n",
    "```\n",
    "conda activate dada2_1.10.0\n",
    "```\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Demultiplexed fastq files for all samples should be downloaded into the `fastq` directory:\n",
    "\n",
    "<font color=\"red\">TODO: Add script to download from ENA once demultiplexed samples available</font>\n",
    "\n",
    "Download the silva v138 database from [zenodo: doi:10.5281/zenodo.3731176](https://zenodo.org/record/3731176#.YVMc-HnTVTY)\n",
    "\n",
    "The following code cell will download the appropriate file and confirm the integrity of the downloaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f290ec80-0d3f-4ae5-bce1-3bf25c50c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import hashlib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "dl_uri='https://zenodo.org/record/3731176/files/silva_nr_v138_train_set.fa.gz?download=1'\n",
    "fn='silva_nr_v138_train_set.fa.gz'\n",
    "good_md5sum='1deeaa2ecc9dbeabdcb9331a565f8343'\n",
    "\n",
    "def file_read(file):\n",
    "    with file:\n",
    "        return file.read()\n",
    "\n",
    "urllib.request.urlretrieve(dl_uri,fn)\n",
    "dl_md5sum=hashlib.md5(file_read(open(fn,'rb'))).hexdigest()\n",
    "\n",
    "if good_md5sum != dl_md5sum:\n",
    "    sys.stdout.write('Downloaded file appears corrupted')\n",
    "    os.remove(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822bdcae-7638-413c-b774-6e3c2accd4b0",
   "metadata": {},
   "source": [
    "## Running the DADA2 analysis\n",
    "\n",
    "The scripts to run this analysis are optimised for use on an HPC cluster running Univa Grid Engine/Sun Grid Engine. It will probably be necessary to modify the GridEngine directives at the top of each script to make them suitable for your compute environment. The scripts are designed for use directly from the command line and can be run interactively, but would ideally be submitted to an HPC system.\n",
    "\n",
    "The process is split into three parts for optimal HPC cluster usage. \n",
    "\n",
    "  *  *dada2_00.R*: QC and read trimming. Runs multithreaded for improved runtimes, using 24 threads by default. To adjust the number of threads change the value of 'num_threads' at the top of the script, and if submitting to an HPC environment then the value of the GridEngine `pe` directive should be altered to match the value of num_threads. Usage information for the script is as follows:\n",
    "  \n",
    "```\n",
    "Usage: ./dada2_00.R [-[-reads|r] <character>] [-[-metadata|m] <character>] [-[-truncF|F] [<integer>]] [-[-truncR|R] [<integer>]] [-[-maxeeF|q] [<integer>]] [-[-maxeeR|w] [<integer>]] [-[-help|h]]\n",
    "    -r|--reads       Path to directory of fastq files\n",
    "    -m|--metadata    Path to metadata file\n",
    "    -F|--truncF      Length to truncate forward reads to (default: no truncation)\n",
    "    -R|--truncR      Length to truncate forward reads to (default: no truncation)\n",
    "    -q|--maxeeF      Maximum expected errors for forward reads (default: 2)\n",
    "    -w|--maxeeR      Maximum expected errors for reverse reads (default: 2)\n",
    "    -h|--help        Display help\n",
    " ```\n",
    " \n",
    " The necessary metadata file is provided in this directory named `Map_JH12_2.txt`. Interim results are stored as serialiased R objects in a `cache` directory.  \n",
    " \n",
    "For this analysis, the script was run by submission to the HPC cluster as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c6dff-0e13-4f5f-a088-68cc28469f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qsub dada2_00.R --truncF 150 --truncR 150 --reads fastq --metadata Map_JH12_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076b340-1ee9-4d89-86f1-76d916514b61",
   "metadata": {},
   "source": [
    "  * *dada2_01.R*: learns error rates, dereplicates sequences and carries out pooled ASV inference using dada2().\n",
    "  \n",
    "    \n",
    "Submitting this script to a GridEngine cluster will run an array job consisting of two tasks, one each for the forward and reverse read which are processed separately for effeciency. dada2 is run for a maximum of 20 rounds or until convergence with the error model. Again, this script is multithreaded, and the number of threads can be adjusted as described above for `dada2_00.R`. Usage of the script is as follows:\n",
    "    \n",
    "    \n",
    "```\n",
    "Usage: ./dada2_01.R [-[-metadata|m] <character>] [-[-help|h]]\n",
    "    -m|--metadata    Path to metadata file\n",
    "    -h|--help        Display help\n",
    "```\n",
    "    \n",
    "In this case, only the metadata file needs to be provided to the script, with results from `dada2_00.R` automatically reloaded from the `cache` directory.\n",
    "\n",
    "The script was run as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad758a4-2e9e-4776-9a3e-50846cf2f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qsub dada2_01.R --metadata Map_JH12_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d7c65-4610-499a-90b5-ce0cf2871481",
   "metadata": {},
   "source": [
    "  * *dada2_02.R*: Merges paired reads, removes chimeras and carrries out ASV classification. A phyloseq object is created containing the resulting classified ASVs and serialised into the working directory as `JH12_dada2.rds`. \n",
    "  \n",
    "  As with previous scripts, the number of threads can be adjusted by altering `num_threads` at the top of the script. Usage is as follows:\n",
    "  \n",
    "  \n",
    "```\n",
    "Usage: ./dada2_02.R [-[-name|n] <character>] [-[-taxonomy|t] <character>] [-[-merge_overlap|m] [<character>]] [-[-help|h]]\n",
    "    -n|--name             Name of job\n",
    "    -t|--taxonomy         Path to taxonomy database to use for classification\n",
    "    -m|--merge_overlap    Minimum overlap for merging reads (default: 12)\n",
    "    -h|--help             Display help\n",
    "```\n",
    " \n",
    "The script was run as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7eafc-2bad-4766-86bf-4f0100fe8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qsub dada2_02.R -n JH12 -t silva_nr_v138_train_set.fa.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-dada2]",
   "language": "python",
   "name": "conda-env-miniconda3-dada2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
